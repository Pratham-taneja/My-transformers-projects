{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "from torchaudio.utils import download_asset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio import transforms\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to find classes in target directory\n",
    "def find_classes(directory: str) -> tuple[list[str], dict[str, int]]:\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "    \n",
    "    Assumes target directory is in standard image classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "    \n",
    "    Example:\n",
    "        find_classes(\"food_images/train\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = list(set(sorted(entry.split('_')[-1].lower() for entry in os.listdir(directory))))\n",
    "    classes.remove('surprised')\n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "    \n",
    "        \n",
    "    # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a custom dataset class (inherits from torch.utils.data.Dataset)\n",
    "\n",
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class AudioFolderCustom(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.wav\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
    "\n",
    "    # 4. Make function to load audio\n",
    "    def load_audio(self, index: int) -> torch.tensor:\n",
    "        \"Opens an audio via a path and returns it.\"\n",
    "        audio_path = self.paths[index]\n",
    "        sample_rate=24414\n",
    "        waveform = torch.zeros(1, 3*sample_rate)\n",
    "        w, sample_rate = torchaudio.load(download_asset(audio_path))\n",
    "        \n",
    "        if sample_rate not in [24414,]:\n",
    "            w = torch.from_numpy(librosa.resample(w.squeeze().numpy(), orig_sr=sample_rate, target_sr=24414, res_type=\"kaiser_best\")).unsqueeze(0)\n",
    "        \n",
    "        waveform[0][:len(w[0])] = w\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        wav = self.load_audio(index)\n",
    "        class_name  = self.paths[index].parent.name.split('_')[-1].lower() # expects path in data_folder/class_name/file.wav\n",
    "        if class_name == 'surprised':\n",
    "            class_name = 'surprise'\n",
    "        class_idx = torch.tensor([self.class_to_idx[class_name]])\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(wav), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return wav, class_idx # return data, label (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.MFCC(\n",
    "    sample_rate=24414,\n",
    "    n_mfcc=50,\n",
    "    melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 53, \"center\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"path/input/TESS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_custom = AudioFolderCustom(targ_dir=dir_path, \n",
    "                                transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_custom.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_custom.idx_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, label = next(iter(data_custom))\n",
    "print(f\"Audio shape: {wav.shape}\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_size=len(data_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(data_custom, [int(total_data_size*0.8), total_data_size-int(total_data_size*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train and test data loaders\n",
    "train_dataloader = DataLoader(dataset=train_set, # use custom created train Dataset\n",
    "                                     batch_size=20, # how many samples per batch?\n",
    "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_set, \n",
    "                                     batch_size=20,\n",
    "                                     num_workers=0,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, label = next(iter(train_dataloader))\n",
    "print(f\"Audio shape: {wav.shape}\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total data: {int(total_data_size)}\",\n",
    "      f\"\\nTrain Data: {len(train_dataloader)*10}\",\n",
    "      f\"\\nTest Data: {len(test_dataloader)*10}\") # batch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.VGG16_Weights.DEFAULT # .DEFAULT = best available weights \n",
    "model = torchvision.models.vgg16(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the manual seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Recreate the input layer\n",
    "model._modules[\"features\"][0] = torch.nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)).to(device)\n",
    "\n",
    "# Geting the length of class_names\n",
    "#output_shape = len(data_custom.classes)\n",
    "\n",
    "# Recreate the classifier layer and seed it to the target device\n",
    "model.classifier[6] = torch.nn.Linear(in_features=4096, \n",
    "                    out_features=7, # same number of output units as our number of classes\n",
    "                    bias=True).to(device)\n",
    "#     torch.nn.LayerNorm([768, 1, 1]),\n",
    "#     torch.nn.Flatten(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary(model=model, \n",
    "        input_size=(10, 1, 50, 456), ## (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss and optimizer\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> tuple[float, float]:\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        \n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = loss_fn(torch.squeeze(y_pred).float(), torch.squeeze(torch.nn.functional.one_hot(y, num_classes=7)).float())\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == torch.squeeze(y)).sum().item()/len(y_pred)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> tuple[float, float]:\n",
    "\n",
    "    model.eval() \n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            loss = loss_fn(torch.squeeze(test_pred_logits).float(), torch.squeeze(torch.nn.functional.one_hot(y, num_classes=7)).float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == torch.squeeze(y)).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    # Adjusting metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> dict[str, list[float]]:\n",
    "\n",
    "    # Creating empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "results = train(model=model,\n",
    "                   train_dataloader=train_dataloader,\n",
    "                   test_dataloader=test_dataloader,\n",
    "                   optimizer=optimizer,\n",
    "                   loss_fn=loss_fn,\n",
    "                   epochs=10,\n",
    "                   device=device)\n",
    "\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from helper_functions import plot_loss_curves\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n",
    "    with open(\"helper_functions.py\", \"wb\") as f:\n",
    "        import requests\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "        f.write(request.content)\n",
    "    from helper_functions import plot_loss_curves\n",
    "\n",
    "# Plot the loss curves of our model\n",
    "plot_loss_curves(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
